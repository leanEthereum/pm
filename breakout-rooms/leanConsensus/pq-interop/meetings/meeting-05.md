# PQ Interop - Meeting 05

## Meeting Overview
**Date:** August 13, 2025  
**Agenda:** https://github.com/ethereum/pm/issues/1675  
**Recording:** https://youtu.be/kyBvoDjbEiw?feature=shared  

---
## Meeting Notes

- **Consensus Mechanism Debate**: Roberto proposed a "perfect consensus" with pre-determined chains (each slot maps to a known block), eliminating fork choice to ensure all validators sign the same message, simplifying aggregation testing. Gajinder and Ream favored 3SF mini with parent chaining for DevNet-0 to track canonical chain and head accuracy, arguing it helps isolate aggregation issues from consensus. Agreed to test simplified consensus first, then add post-quantum signatures (PQSig).
- **Tooling and Genesis Generation**: Discussed key generation for DevNet-0 (50-byte keys, ~2 seconds for 2^10 slots, potentially hours for 2^18). Proposed pre-generating keys for longer lifetimes (stored in S3) vs. on-demand for short runs. Need forks of Ethereum Genesis generator, eth-beacon-genesis, eth2-val-tools, and Ethereum package. Dora support may be deferred; Prometheus metrics suggested for initial monitoring.
- **Spec Repo Updates**: Reviewed PRs (#5, #7) for 3SF mini containers (using zero hashes instead of optionals for SSZ). Plan to create markdown files for container definitions to facilitate discussion before integrating into Python framework.
- **Validator Count**: Initial proposal of 1000 validators questioned due to key generation constraints; suggested starting with 100 validators for DevNet-0.
- **Communication Channels**: No update on Discord-Telegram bridge (carried over).
- **Broader Outcomes**: DevNet-0 deadline (August 31) appears aggressive due to tooling needs. Plan to draft a readiness plan to assign tasks and assess timeline impact before October interop gathering.

**Action Items**:  
- Draft and share DevNet-0 readiness plan for task assignments (Will, consult Barnabas/Gajinder/Ream).  
- Create markdown files for 3SF mini container definitions and finalize PRs #5, #7 (Ream/Felipe).  
- Benchmark key generation times for 100–250 validators at 2^10 and 2^18 lifetimes; develop independent key generation tool (Dockerized, Rust-based) (Benedikt/Gajinder).  
- Explore forks for Genesis generator, eth-beacon-genesis, eth2-val-tools, and Ethereum package (Gajinder/Camille).  
- Update on Discord-Telegram bridge setup (Will).  
- Circulate meeting minutes and recording (Will).  

---
## Meeting Transcript

### Introduction and Greetings
**Will:** There's the agenda. In the chat. I'll give it a minute or two. Okay? Everyone. Welcome to this week's PQ Interop Call. The goal of this call series is to serve as a touchpoint for researchers and client devs during the iterative process of developing, deploying, testing, and improving a series of devnets focused on aggregating post-quantum signatures. And this breakout room supports the longer-term goal of lean consensus vision. Let's dive in. I've got the agenda in the chat. I think that there are some discussion around perfect consensus. Roberto, thank you for staying up to the wee hours of Australia's evening. Is there anything that you wanted to start off with?

### Perfect Consensus Proposal
**Roberto:** I think mainly just probably repeating what I wrote in the message on Telegram. So my idea was that because of the scenario where we essentially have a synchronized clock, we can essentially manufacture consensus by having each validator at the beginning of a slot to sign a message depending only on the number of this slot. And in this way, essentially, all validators always agree on what to sign. And in this way, we don't run the risk that, for example, if we use some form of fork choice, perhaps some validators are a bit behind and they vote for different blocks than other validators and we cannot aggregate signatures. So this was overall the idea behind what I wrote on Telegram and theoretically the reason why we can do this is that essentially we have a trivial consensus algorithm, meaning that essentially there is only one value possible that can be decided, which is essentially the number of the slot. That's why we can do this.

**Will:** Roberto, Gajinder threw his hand up pretty quickly. Maybe we'll take Gajinder, I don't know if you've had an initial question on setting context for this.

**Gajinder:** So, Roberto, as you mentioned, consensus has to be pretty light and simple. And clock is synchronized anyway. That is the basic assumption behind the beacon protocol as well, that the clock is synchronized. So that is what we are aiming for. What we discussed with Ream, and we believe that chaining is sort of important. And once you have chaining, some sort of finalization is also important because otherwise the clients will become unstable over a long period of time. And it will become too fork-y. So with that regard, we basically decided that we will go with just chaining, basically having parent, child relationship and validation of that in the consensus. And for fork choice is basically we wanted to go with 3SF mini because it looks simple enough and Zeam was able to implement it pretty quickly, and I believe Ream is also able to do that. So with regard to that, I believe our choice of chaining and 3SF consensus is pretty minimal and simple with regard to kickstart this devnet where we don't think that a lot of time will be spent while running the consensus. And it should be pretty trivial, not in terms of implementation, but in terms of client running, proposing, and basically running the state transition function. It should be pretty simple entry for the client. Also, it is simple enough for teams to implement. I mean, Zeam and Ream both have it. So in that sense, we don't see any challenge over it. And for block production, definitely we will do a round robin based upon the slot. So that has already been discussed. So in that sense, we totally are on board with your proposition of keeping the consensus simple. And we believe that chaining + 3SF + round robin block proposal is giving it simple.

**Roberto:** Thank you. So my proposal is essentially to eliminate consensus. So there is no... my proposal is consensus so simple that there is no message sent to arrive to any consensus. But if there is no consensus, so that way we won't be able to calculate which one is the canonical chain, for example.

**Gajinder:** The reason because it is important, because, for example, if the aggregation is late or if voting is late, then, for example, canonical chain would be different. So there should be forking, right? So if you try to interrupt... sorry, it's interrupting because I understand, but what I'm saying is that tracking chain is important to figure out how efficient the scheme is, whether all clients are able to, at what time? So basically taking all these delays and coming into an aggregate number of attestation effectiveness, because head vote accuracy is the most important metric that we need to track. And with that regard, I think chain is pretty important to have. Isn't the objective to measure the performance of the signature aggregation? That is the objective, right? We want to see major sub-metrics, like how long it takes us to aggregate a given number of signatures, plot in the graph, right. That we don't actually need a devnet. The objective of DevNet is to run post-quantum signature scheme in entirety and to end and see what are the challenges and what are the metrics that are out there, which include the time for signature aggregation, which all the clients will be doing that. But just for the purpose of benchmarking signature aggregation, we basically don't need a DevNet. Kamil basically is also running some sort of simulations, right? So we will have some benchmarking results, even from simulation itself. With that regard, I think there are other paths to do benchmarking and to figure out whether signature aggregation is efficient enough or not, or if, for example, there is some particular signature size, what is the expected delay that we can expect on a simulated network and through network simulator and all these tools. I think we will get a good idea about that. But post that, we basically want an end-to-end measurement of what this post-quantum signature does not look like, including, and now this is not just, for example, about signing and aggregating. It's also about, for example, Genesis generator or Genesis generation and keystore generation, because now the validators will start from a seed and they will need to generate private keys and sort of come to public key. So basically, overall process. So it is sort of an end-to-end, this particular experiment that we and Ream sort of thought about, and we basically started on it. So aggregation is, I would say, part of it, although very important part of it. But what we are trying to go over here is basically see how it really works out in action. So which are the parts in addition to... I mean, like, in addition to aggregation, what are we trying to measure?

**Roberto:** Sorry, I missed that.

**Gajinder:** Yeah, so we basically need to figure out what is the best form of Genesis generation. Although it is generally for testnets and from mainnet it would be half-baked into. But the point of that is how, for example, a post-quantum key would be generated and registered. That could be a later stage registration, could be a later stage goal, to see how it works out in that field. But at least, for example, if you want to start up a validator and you have a keystore, how much, for example, lead time you need to generate your private key and to basically start up the validator. I mean, all these things are logistical things, but they are quite important. Another thing would be measuring signature time, measuring the bandwidth in the real scenario, and as well as what are the delays that are happening, right? So measuring the latency and measuring the hops, whether we can improve gossip sub, what are the delays, what are the hops that we are getting with the current gossip service there. Another great, for example, there is square grid proposal that is out there. Is there another grid that we can basically have so that we can propagate faster? Then aggregation is definitely one of the points, and the verification of signature is definitely one of the points in that entire scheme. So the consensus of chaining and 3SF finalization, that is, I think, very small bit, and we don’t see it as a computational or resource hurdle at all.

**Roberto:** So my concern is that it’s not so much about the complexity, it’s that because it’s possible that if there are delays on the network, different nodes on the network have a different view of the tree, and therefore they send votes that are different. They cannot be aggregated. And then if this happens, you don’t know if the aggregation doesn’t... integration, if you don’t get enough aggregation, because the aggregation process has some problems or because the consensus protocol is a problem. So my main concern in all of this is that we have two moving parts: one is consensus, one is aggregation. And if aggregation doesn’t happen, we don’t know if it’s because of the question, problem with aggregation scheme, with the infrastructure of aggregation, or the infrastructure of consensus.

**Gajinder:** I understand where you’re coming from, but we believe that consensus is pretty simple and simple to understand and implement with regard to just chaining and 3SF, so we don’t see much complication arising from that. To be honest.

**Roberto:** Sorry, I’m not saying at all that it is complex. That’s not what I’m saying. I’m saying you implement the consensus, and it’s simple, but consensus does not guarantee you that nodes will vote for the same block. Because if there is bad network condition, you can end up with nodes having different views to chain, and therefore they don’t send messages that can be aggregated. You see what I’m not speaking at all about simplicity. It’s about the problem that you can have.

**Gajinder:** I understand, but having a consensus is not complicating that problem, right? So having a consensus will basically help us figure out whether the problem is because the nodes have a bad view of the network, right? So consensus will tell us whether there are some nodes who are, for example, on a fork. Rather than it would become more complicated and sort of more messy if we don’t have a consensus and we just try to generate metrics based upon just signatures and signature aggregations. Right? So I believe that having consensus, basically having a definition of canonical chain, right? Why we need consensus? Because we need a canonical chain to figure out what is the head accuracy and why. Are we using 3SF again? For the stability of the network, it’s not really required. We can also junk it if we see complications in that. But the basic thing is that this is a very simplified consensus that we believe will actually help us differentiate between whether the performance issues or the degradation in metrics are rising out of consensus issues, or they are arising out of some other libraries that are out there, or performance of the aggregation or verification. So in that regard, we believe that our choice of this particular consensus will actually help us. But if it won’t, we can always junk it and come back to it. I mean, it doesn’t stop us to do what you are proposing. But I believe that it would be easier if we have a well-defined canonical chain that we can track and we can basically define our metrics on.

**Will:** Roberto, I guess in the spirit of doing iterative Devnets, are you suggesting that we should... it sounds like you have concerns about coupling the signature aggregation with consensus. Are you just suggesting that we should increment our way towards combining those two? Like, start with just consensus and then layer on signature aggregation?

**Roberto:** No, I’m suggesting we should... so in a way you can see if you can say that what I propose on Telegram is a consensus. Essentially another way to look at it, we can have a pre-defined chain. We can have a chain which we already agree on, which is pre-minted. We have a block for each slot and each slot you propose exactly the block that everybody knows, or everybody votes for the block that it’s already agreed up front. Like imagine a Genesis and I don’t know, I run this DevNet for 10 days or whatever. We already have a pre-determined chain. Every node knows exactly the chain that they will vote for. So every node already agrees. That’s the thing. Like, there is consensus. It’s just implicit. And because of this, I think it’s simple. But that’s not the thing is, you are not going to end in a scenario where nodes disagree. But if you implement 3SF or whatever, it is possible they know disagree. And now if nodes disagree, they cannot aggregate these signatures.

**Gajinder:** In that regard, I disagree that in absence of consensus, it would be difficult to establish whether the nodes agreed or not. So that is my response to that. I think we should also hear from Ream Team, what their thinking is and as already pointed out, we will basically first make sure that we are able to interop just with our simplified consensus and then bring in signatures to it so that we can deal with complexity properly.

**Will:** Kamil?

**Kamil:** Yeah. Sorry. Maybe a stupid question, but in 3SF mini, are we going to have multiple aggregations happening at the same time? Because some nodes voting for current round and others for the previous round. Is that correct? So basically, the aggregators will basically aggregate based upon the head data, right? So this is how aggregation generally happens in beacon consensus as well. The votes are aggregated based upon the target based upon the data that you are aggregating on. So similarly for some, but some nodes are voting for the source, some votes are not supporting for the target. Is this happening at the same time? So in mini 3SF, so basically the votes are generally for again, for the head, source, and target as well. So I think all three need to be the same for them to be aggregated. Yeah, just my concern is that if it’s happening at the same time, then it’s probably... is that realistic? How it’s going to be eventually? Or we will have different sets of validators voting for different rounds? Because in reality, we have multiple...

**Gajinder:** Likely the latter. So in high latency situations, for example, different validators will have different views on what these source, target, head triples are. And it’s actually important for incentivization that even if, for example, your view of the head is incorrect, but you still have the source and target correct, that you still get a reward for attesting. So yes, the short answer is we will be aggregating over different messages.

**Will:** Oh?

**Ream:** Yeah. So I just want to point out again what I typed that so we are planning to do the mini consensus first and then mini consensus plus the PQSig. So hopefully that should de-risk figuring, being able to troubleshoot issues that are blurred between the two. So that’s the first thing. The second thing is I actually have a question on if we are going to go ahead with Roberto’s proposal, if we don’t have chaining, so basically only had to slot number, but without the parent, then wouldn’t we... aren’t we going to be only aggregating the latest view, but we won’t be able to figure out whether it’s the same chain? Like some validators might be missing some heads, some are not. So I think that might actually make it harder to get a shared view. And the third point, the last point I want to make is I think we can actually measure separately between the signature validation that the signatures are correct. We can actually measure that separate from the actual consensus. So if we are concerned with just measuring the PQ signature’s performance, then I think we are able to do that even though we don’t have consensus on what the content of the attestation is.

**Will:** Roberto?

**Roberto:** Yeah. Thank you. So to the second point, correct, you are asking whether what I’m proposing could be that validators from behind or they don’t agree, don’t see the same thing, right? But in order... that’s just impossible. We can have a chain, we can have a chain, imagine we pre-compute a chain. We can have a function that for each slot gives us a block with a parent, the block from the slot before. And every validator knows exactly what to vote for, for a given slot. So this is a... it’s just that it’s determined. And we can do this because in what we are building, we don’t care about having actually actual transactions. So, like, we can precompute the entire chain, and each validator knows exactly what to vote for in a slot, meaning they’ll agree by default. It’s impossible that they will disagree. So I have a question: if we can have a perfect consensus where at every slot you are guaranteed, despite even if there is complete network asynchrony, very high delays, that all validators will vote exactly for the same chain, and you have another consensus that doesn’t guarantee this property, why would you not choose the first one? So we actually did have this as a proposal, and that was for EFCC, and it is definitely possible to have a devnet with this perfect consensus, which is super simple, and then move on quickly after. My take is that I don’t have a strong opinion on what we do, but if we were to have this perfect consensus, then we probably don’t need it for very long, and we can move over to something more complicated pretty swiftly. And it seems like the teams that are building this, like Ream and Zeam, basically have a preference to just go directly with something a little bit more sophisticated. And you’re right that if you really want to isolate the testing of the signature aggregation, it’s possibly better to go with this perfect consensus. But I think, as Gajinder tried to explain, is that there’s all sorts of other things that we need to test related to consensus and fork choice, rules and whatnot. Ultimately, it has been considered a couple months ago, and I don’t think it’s a super important kind of decision at this point.

**Ream:** If I may, I want to respond to Roberto a bit. So on pre-determined mining the chain, I would actually kind of question whether we’ll actually need interop for that. It sounds like something that each of the clients would be able to run on their own and give us the performance measurements there. But however, I think it might be a good precondition before going into actually running the interop devnet that each of the clients is able to sign and attest to their own predetermined chain or even determined chain that’s shared between the two clients, but run separately, before actually committing to or starting to interop of them. I mean, I think it could still be a value from for interop even if you go to this perfect consensus, as you might want to see the signature aggregation implemented by different teams, they combine together. Potentially, it could be a reason there still to check how different implementations of the signature aggregation algorithms work together.

### DevNet-0 Tooling and Key Generation
**Will:** Well, I dropped a note in the chat reminding folks that we sort of self-assigned the DevNet-0 to be due August 31st. That’s two weeks from today, approximately. Is there anything that’s been brought up in this discussion that sort of undermines that set of objectives? It looks like in the chat, Ream is saying signature chaining is sort of infers... I think we meant parent chaining. Okay. And then there are significant work we need on tooling Genesis generator Kurtosis. So this looks aggressive. Yeah. Maybe on that note, let’s put a pin in this topic for right now. And Gajinder and Barnabas, I believe you guys had a conversation about some of this tooling yesterday. If you could maybe give us an update, let us know where we stand with that as a dependency.

**Gajinder:** Yep. Me and Barnabas had a discussion over this, as well as some discussions with the team async. I will let Barnabas take the lead on this.

**Barnabas:** Hi, guys. I’m very much from deep in love stream. So we’ll give a quick update on what we need every time to basically get up to speed. Docker images for every single client that want to participate in this DevNet-0. We can add support for our automatic client Docker image builder. And we’re going to be able to build it and apps images for any of your clients. We’re going to need a fork of Ethereum Genesis generator. This fork will be able to create the genesis files for whatever file format you’re proposing to have the underlying library. Here, that also needs to be updated is the eth-beacon-genesis. Including links for all of these. The eth-beacon-genesis is basically the underlying library which creates the SSZ format for these clients. This is... the structure is probably going to be different for this DevNet-0. So this has to be created and in order to support the different multitrack key structure, we’re going to need to have a different fork of the eth2-val-tools. So eth2-val-tools is the one that generates the current private and public keys for all the different client validator clients. Ideally, we need to figure out how long does it take to create this post-quantum signature, whether we want to generate all the validator keys in advance and shove it in an S3 bucket and pull it on demand, or we want to generate them on demand with Kurtosis. We can come back to this later. And then lastly, we probably want to maintain a fork of the Ethereum package, which will give us supporting test all these clients locally on your local machine. But yeah, if you want to have visibility, we probably also need to have a fork of Dora. This is a Go project, like a chain explorer. It probably can have a shortcut and the whole world infrastructure is going to be different post-quantum, so it might be quite some work to get the Dora up and running. You might just have to use the Prometheus metrics for initial DevNet-0 and maybe have the Dora support for later than that. But all the other ones would be appreciated with it in order to get DevNet-0 up. Regarding the open questions, mainly the validator key generation, whether we want to do this in advance or on the fly. Would anyone have experience in generating these validator keys with this new library? How long does it take to generate one? And how big the key is and how long would it take to generate 2000 locally on the machine? And how big they are?

**Will:** Any takers?

**Benedikt:** I mean, I don’t know for sure, but I think the longer it takes, it all depends on how long we’re going to run the Kurtosis chain. Basically, if it’s just a few minutes, we don’t need to spend a lot of time generating the keys, right? Then it’s probably good design on the code.

**Will:** Ten days.

**Benedikt:** The size of the keys is about 50 bytes, if I’m not wrong. So, like, one Merkle root plus some five field elements, maybe. So it’s like, yeah, 50 bytes. Okay? So tiny file, but how long does it take to generate a key? If you want to have four hours to generate one key, four to five hours, in the order of hours, I’m not sure. Of hours, okay? You are flexible if you generate the keys. You don’t have to go to the maximum lifetime, which is 2 to the 32; if you do that, then it will take you not hours but days. If we all agree that we do 2 to the 18, then I suspect you can do it in a few minutes. But I would have to rerun the benchmarks again. And if we want to have, like, one hour runtime, because for Kurtosis, it doesn’t really make sense to run more than one hour, then it could be done. One hour to generate one key or two? No, one hour runtime, not ten days, but even less slots for a single key. Yeah, I can check that. But do we have a tool for this? Like the key generation tool? We have, like, a Docker container for it? We can, like, start playing around with it?

**Gajinder:** No, I mean, there’s a Rust library. That’s all we have. But I can check the key generation time. It would be nice to have some metrics.

**Benedikt:** Yeah, maybe, you know, out. If you had... we just need, if we need to run a chain just for an hour, so it’s just every four seconds, it means 900 slots. If we just need the key for 900 slots, like how long it takes to generate such key? That would be extremely fast. If you just want 900 slots, that’s not even 2 to the 10. Yeah, I mean, I have some old numbers, and numbers should be better than that. So for that lifetime, 2 to the 18, so that’s already much more than 900 slots, that should be 200, 270 seconds, maybe. Yeah, that’s a few minutes. Can it be parallelized? Can we generate... it is, it is already parallelized. No, but you’re talking about one key. What if I want to create 1 million keys? You can do that independently, but already generating one key is already internally parallelized. Okay? Okay? So in theory, it should take the same amount of time if you generate 10 keys if you have enough cores.

**Gajinder:** Oh, sorry, the number that I gave you was for different hash functions. So let me revisit that. Yeah, still 330 seconds for a single key, but that’s old numbers. So right now it should be even faster for lifetime 2 to the 18. Because we improved the code since then. Yeah, so I can give you more accurate numbers after the meeting. You can just ping me. So, ideally, what we want is we want to have a new validator key generator where we can set what kind of lifetime we expect Kurtosis is done not to have, and then we can probably generate them on demand. And if it really just takes up one second to generate it for like 900 slots, it makes sense to just have a new key generated for each run. And then for larger devnet like DevNet-0 when we are talking about longer lifetime, it’s probably gonna make sense to have a beefy machine. We’re going to be pre-generating all the keys, we’re going to shove it into an S3 bucket, and we’re going to be pulling the validator keys on demand when we spin up DevNet-0, and we don’t then have to wait multiple hours to generate these longer lifetime keys.

**Will:** Barnabas, just to be clear, this is not work that PandaOps has the bandwidth to do. We’re looking to assign that to this group.

**Barnabas:** Yes. So currently our primary focus is on Fuzaka work before we can ship Fuzaka. And we’re going to have best effort after Fuzaka.

**Will:** Sure.

**Barnabas:** But yeah, feel free to fork any of the repos and ask for input, and then we’re going to try to get back to you.

**Will:** Camille, in the chat, the Telegram chat, just prior to the call, you had mentioned maybe joining the party with building a client implementation. Just curious, like hearing this, I don’t know, I’m just kind of, like, thinking on my feet, but would you be in a position to help set up some of this overarching infrastructure in lieu of doing that? Or would we prefer to have a third client implementation and kind of disperse this work amongst everyone?

**Kamil:** Yeah. You mean this kind of infrastructure? You mean Kurtosis and all of that stuff?

**Will:** Yeah, Kurtosis, Dora, the signature generation, or all of the things that we’re going to need, basically to be able to bootstrap the first devnet.

**Kamil:** I don’t know. I never touched the Kurtosis, so I need to check it out.

**Will:** Okay? Sure.

**Kamil:** Before answering. By the way, Kurtosis would be like step five, and that is the part where I can actually have quite a bit on all the other, like the underlying libraries and all the prerequisites would be in the main heavy lifting here. Genesis generator, I think I and Gajinder have some experience in dealing with that, and maybe we can sort of take a look into it and come back whether we can support it. But I think coming back to the logistics of key generation, what I suggest is that we have that as a separate step which is not really integrated, because then, for example, somebody in the chat noted that we can maybe try to use the same generated keys over and over again for local testing. And I think that is a very good idea because we wouldn’t want to wait minutes for a local devnet to startup. It would get frustrating. So in that sense, I think it would make sense to sort of separate it out as a two-part process where the key generation happens prior to the Genesis generation, and the Genesis generator can just use the generated keys, the generated keys config to generate the genesis, and those generated keys can also be easily distributed among the validators or validators can just pick and choose, and this would also tie very well into the long 10-day long running devnet where we would just generate the keys and pull them from the buckets.

**Benedikt:** Just to chime in here, just rerun the benchmarks for the most up-to-date code, and for generating 2 to the 10 keys, because we said 900 slots, it took, like, two seconds to generate one key, right? But we would probably want to run around 120 to 250 keys at least, then number and multiply it by... right, but that’s the question, like, how parallelizable is it to generate 200? Like, how long does it take to generate 200 keys parallel? I mean, the key generation is internally parallelized, so I’m not sure how much you would gain from just paralyzing over the different keys, right? That’s the main question here, I think, because if it’s two minutes, that’s too much. If it’s 20 seconds, that’s too much. If it’s two seconds, then that’s something we can deal with. I think anything over two to five seconds is too much. If we at any point go to realistic lifetimes, then it will for sure be much more than that, like, yeah, but with these schemes, for realistic times you expect like hours of key generation, right? And for that, we can always use the pre-generated keys. I see, I’m just saying that fetching on a Kurtosis network, fetching a file from remote might take longer than generating it locally. Then at that point we might as well generate it locally, and that is going to allow us to also work offline. Like right now, one of our main selling points is that we can basically run a whole Kurtosis enclave offline, we don’t rely on any external dependency, we don’t rely on the Internet. So ideally, we want to keep it that way, especially if it’s faster. And this is why I think we need an independent tool that does the key generation, where we basically take two or three inputs: one is going to be how many keys we want, the second one is going to be what kind of lifetime we expect, and the third one could be, like, where do we want to store the key files? That’s it.

**Gajinder:** Okay, I guess that can be created just from the library. I mean, the keygen function already has this. Yeah, exactly. So just have a library for it at a Docker file, and add that tool to our image builder, and then you’re going to have an EthPandaOps post-quantum key generation tool. And from that point on we can integrate that into all the other testing stack as well.

**Benedikt:** One thing worth noting is that if you pre-generate the keys, you can have the public key be pre-generated and all of the private stuff can be subsumed to just your seed, and then you can basically regenerate it in real time. So you can generate the actual secret stuff, but the stuff that you need to sign fast, you should not, like, you cannot generate. That would take as long as generating the key in the first place. So I would not throw that away, I would store that. Well, you can still basically maybe a layer that’s halfway through, and then that’s like square root the size or something like that. Yeah, right, in practice, I would just store the entire key. How big is this key? One little letter, the whole thing, that again depends on your lifetime, right? Oh, there’s actually so, like, 2 to the 18, yeah, times something. Okay? It’s an entire Merkle tree with 2 to the 18 leaves. It could be actually gigabytes of data, yeah, for sure, for sure. Yeah, you can throw it away, but then signing is incredibly slow, and I guess we won’t sign these fast. Yeah, so this is where the square root idea comes in. You only store, let’s say, 100 kilobytes, and that’s enough data, and you can still sign every block by computing a little bit locally. Is it such a problem? Is the secret key is that big? Why do we care? I mean, validator space Yiannis anyway, right? No, but if you run up, for example, local network and you need to fetch each time you run something, you need to fetch 500 gigabytes of data, then there’s going to be a why. Sure we can do it one-off, but I’m thinking here, local test, we are home network, for example, you want to run 500 validators, you’re going to need to fetch 500 gigabytes. That’s significant.

### Spec Repo Updates
**Will:** Okay? Great. I will do my best at synthesizing this conversation into a preliminary DevNet readiness plan of things that need to be accomplished and try to socialize that so that we can get people to volunteer for who’s going to take what and try to assign some time to this and see what sort of impact this is going to have on the overall schedule. A lot of people are gathering in October for a small interop, and we’ve got about seven weeks from now until then. So I still think the goal is to get this infrastructure in place and have already done a single devnet prior to that gathering, if possible. So I will try to get this pulled together here in the immediate future. We’ve got 12 minutes left on the call. Oh, I will drop the links that you had on the comment, the two PRs that you wanted to talk about. If there’s anything here that you want to call attention to.

**Ream:** Oh, yeah. Maybe a quick update from our discussion last interop.

**Will:** Yeah.

**Ream:** I think you mentioned that Zeam was using Union for the optional field on the SSZ, but I think I looked into, like, Zeam implementation. That looks like you’re also using zero hashes instead of optionals. So love confirmation there. And if you’re using zero hashes, I think that’s great because Ream is doing that, too, and it’s a lot less complex to implement.

**Gajinder:** Yeah, we are also using zero hashes on that. Basically, we had discussion on this, and we got clarity on what exactly you’re asking. But with regard to your PR for containers, can we sort of have that MD file like we have in the main consensus specs and basically go start the container definition from there? Right now, I think you are... basically, we have some comments over there, but we can do on the PR once we have MD container definition file, because commenting just on the Python code doesn’t seem appropriate for a discussion.

**Ream:** Yeah, definitely. I can do the MD file. But maybe a question to Felipe: do you have any thoughts on that? Because I think you wanted the specs to be in Python and follow your framework, right?

**Felipe:** The specs... yeah, I don’t have a whole lot of context on how we want to organize this. I think we need to make a decision, kind of as a group. I kind of commend just to make sure we’re using all the same tooling across the specs. But I’m sort of new to how we want to organize it as well. So one thing we need to discuss together, I think.

**Ream:** Okay, cool. Makes sense. I think in that case, I’ll create a markdown file for Ream and Zeam. We can agree on the markdown files, and once we agree on the specs, happy to try out and to put it into Felipe’s frameworks. But yeah, I’ll start out the markdown files.

**Felipe:** Yeah, I did make a comment this morning on there. I reached out to you, Tamaghna, just to see if you had any opinion there. But if we could move them out of where the Python is and maybe into docs for now, and we can think about how to organize it beyond that as well.

**Will:** Great. Yeah, I was just going to throw it back to you if you want to discuss those PRs.

**Ream:** So I’ll get back to the PRs, turn the code into markdown files, and then I’ll let everyone know in the interop group.

### Validator Count and Wrap-Up
**Will:** Great. In the chat, let us last drop the note as far as I know, we discussed minimum thousand validator devnets in Cannes. Given the key generation constraints, should we consider lowering that number to the low hundreds, Kamil? And I guess we could start with 100, at least initially.

**Kamil:** Does it matter? Because I’m 10, I don’t think the number matters much as much as just getting the basic steps out of the way. Figure it out. Changing 1 or 10 or 100 doesn’t really matter.

**Gajinder:** Yeah, I think once we have all the tooling in place, we can easily sort of experiment with this and scale it.

**Will:** Sounds good. The remaining time that anyone else have anything else? Did anyone have anything else that they would like to discuss? Okay? Great. Like I said, work to a synthesized note and first draft of a readiness plan. I’ll probably run that by you, Barnabas, and maybe Gajinder or Ream first, and then just open it as a PR so that everyone can comment on it and get a sense of who’s going to take what. All right. Thank you all. Talk to you soon.

**Participants:** Thanks, everyone. See you guys. Bye. Bye.