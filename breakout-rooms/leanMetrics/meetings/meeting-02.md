# CL/EL Metrics - Meeting 02

## References
- [Ethereum Metrics Repositories Structure](https://hackmd.io/@katya-blockchain-dev/ethereum-metrics-repositories-structure)
- [Beacon Metrics Repository](https://github.com/will-corcoran/beacon-metrics/tree/master)
- [EthPandaOps Data History](https://ethpandaops.io/data/history/)
- [EthPandaOps Xatu Data](https://ethpandaops.io/data/xatu/)
- [Consensus Specs Viewer](https://jtraglia.github.io/eth-spec-viewer/)
- [Ethereum Improvement Proposals](https://eips.ethereum.org/)
- [Beacon APIs Changes](https://github.com/ethereum/beacon-APIs/blob/master/CHANGES.md)

## Meeting Overview
**Date:** August 08, 2025  
**Recording:** https://drive.google.com/file/d/16kTXULawWqOi6CISfdI39Cf5Cy6CRNJ1/view?usp=sharing

---

## Meeting Notes
- **Introduction and Repo Demo**: Katya introduced the initiative to standardize metrics for consensus layer (CL) and execution layer (EL), started during an EP fellowship. Demonstrated the repo structure ([GitHub: beacon-metrics](https://github.com/will-corcoran/beacon-metrics/tree/master)) with sections for mainnet (production metrics by functionality, e.g., beacon chain, validator), development (by specs, hard forks, features), and research (nice-to-have metrics, templates for issues).
- **Researcher Perspectives**: Discussed bottlenecks: changing specs, coordination, and naming conventions. Emphasized uniform metrics for identifying bottlenecks and long-term monitoring. Suggested distinguishing one-off vs. standardized metrics for repeated research use (e.g., request metrics). Highlighted challenges in precise research metrics (Prometheus limitations) and the need for detailed logging or tracing.
- **Organization and UI Suggestions**: Proposed dropping network-specific naming (e.g., "mainnet" to "metrics.md"), using EIP numbers for files, and splitting client tables for clarity. Suggested YAML for structured data, static site generation ([e.g., Consensus Specs Viewer](https://jtraglia.github.io/eth-spec-viewer/)), and CI checks for maintainability. GitHub preferred over GitBook for accessibility and contributions.
- **Naming and Standardization**: Debated Prometheus naming conventions (e.g., unit suffixes) vs. client-specific naming. Proposed aliasing or re-exporting via tools like EthPandaOps' metrics-exporter for existing metrics, with standardized naming for future metrics. Noted importance of precise trigger conditions for research reliability.
- **Implementation Challenges**: Highlighted costs of metrics (e.g., per-peer metrics in Nimbus), suggesting feature flags for verbose metrics. Proposed working groups for client buy-in, PR reviews, and outsourcing contributions. Suggested focusing on few well-defined metrics with detailed descriptions to avoid performance issues.
- **DevOps and Data User Insights**: Emphasized dashboards (Kurtosis/Grafana) for client comparisons ([EthPandaOps Xatu](https://ethpandaops.io/data/xatu/)), simplifying analysis, and monitoring post-mainnet (e.g., PeerDAS). Stressed searchable metric lists with descriptions and data sources for data scientists.
- **Broader Outcomes**: Informal discussion on iterative repo reorganization and stakeholder engagement. Plan to avoid overburdening clients while ensuring useful metrics. Scheduled follow-up meetings with stakeholders (e.g., Dune, Nansen, EthPandaOps on Aug 15).
**Action Items**:
- Reorganize repos (e.g., use YAML, static site, drop network-specific naming) and seek feedback via PRs ([Host/Katya](https://hackmd.io/@katya-blockchain-dev/ethereum-metrics-repositories-structure)).
- Open issues in client repos for EIP-specific metrics (Will).
- Form working groups to agree on standardized metrics (All).
- Create templates for research issues and dashboards for metrics (Katya/Researchers).
- Engage external contributors for metric lists and reviews (Katya).
- Schedule follow-up with Barnabas and others (Aug 15, 29, Sept ACDT) (Will).
- Explore Prometheus/OpenTelemetry aliasing and EthPandaOps metrics-exporter for naming consistency (Raúl/Minhyuk).
- Circulate meeting minutes and recording (Katya).

---

## Meeting Transcript
### Introduction and Greetings
**Will:** Will, did you reach out to Maria or no?  
**Katya:** Yep. Yeah, she's going to make it. Okay, great. Hello. So while we are waiting, I just share the link, the couple of links. So what we are going to discuss today is this structure. And we have a, we will have a demo. Hello. So from the outside show. Hello. And probably. Nico will join us from CL part. And I guess that's it. So I think we can just start. So for those who don't know me. Yeah, I started the initiative of standardizing metrics last year during my EP fellowship. Originally, it was an office initiative and they created the repos back in 2020, I guess. But yeah, it’s kind of a little bit stuck. So yeah, it’s been time-consuming to manage these repos. But we are moving forward and we are trying to organize these repos. Now we have repos for execution layer and consensus layer. And we are trying to talk to different parties. So how to better organize it and we would like to hear from you. So I’m going to make a short demo of what we have. Will, I used your repo originally. So I’m going to show you what we have now and what’s the idea. And then I would like to hear from you. It’s just an informal conversation with all the parties. Not like an official call, but would be great to hear from people who can use these repos.  

### Repo Demo and Structure
**Katya:** So let me share the screen. So here's the example of beacon metrics, which we still discuss. Probably it will be execution metrics. So. The readme just explains how it works. So we won’t stop on this. And we have three main parts. Development, mainnet, and research. So let’s start from mainnet. Mainnet is for metrics which are in production already. And this is a readme file. And we will have all the metrics, maybe in one file, maybe in a few files. But we will organize them by functionality. So the idea is to have metrics in mainnet by functionality. For example, this is an example for beacon chain. And it can be like beacon metrics, validator metrics, fork choice, etc. So with navigation, let’s say we have pure metrics. And we have the naming type description and kind of statistics, which client has this metrics. So. And so this is the idea. And metrics can duplicate each other. For example, if they are both related to slot and epoch, just for easier navigation. And probably potentially, which we can discuss. Let me find the example. So for example, if you can’t rename your metrics in production, probably you will just put your metric here. Like clients. There should be an example. Oh, here it is. So for example, if you don’t have beacon reaworks, maybe you have something similar. Just would be great to know. So this is the main part. Then we are going to have development part. Let’s start from research. For research, the idea is to have files like nice-to-have metrics, the list of metrics. For example, currently we have a Glow net HackMD article. Like this. The metrics will be great to have for research. And we may have this MD file here with the list of metrics. Sorry, I didn’t get time to brush it up. We may have all these metrics here. And for example, a researcher can open an issue, which metric it has for these. We are going to use kind of templates. We will create templates like motivation, the list of metrics. Just for researchers, easier to use. And after that, we can create the more properly the type of metrics, the description, all these tables you have seen. And this will be in the research part. And for the development part, we were thinking of organizing it according to the specs. For example, for beacon chain, it will be. And division into like hard forks, for example, and features like possible just for easier navigation. For pool, we may have it by APIs. Some statistics over there and the same like which metric is in progress. They can be organized also by functionality. Let’s say beacon network, etc. And so we are going to open PRs here for current development metrics in development. So you can easily attach them to your issues. If you have big issues for EIPs, just to follow the metrics and etc. So yeah, the idea is like this and we would like to hear from you. For example, let’s start from researchers. Because we think these repos are important for researchers, for developers, and for expand-up steam, yeah, for testing. So yeah, I would like to start from researchers, maybe. So maybe, you know, you have been utilizing a lot of these metrics with PeerDAS. And like trying to have uniform metrics across all client implementations helps identify bottlenecks and speed up performance testing. If you will also, at some point, want to speak to how these are utilized and why they’re important. That would be great.  
**Jae Hee Lee:** Hey, great to see more people in this meeting 🙂  
**Katya:** [Shares link: https://hackmd.io/@katya-blockchain-dev/ethereum-metrics-repositories-structure]  
**Katya:** [Shares link: https://github.com/will-corcoran/beacon-metrics/tree/master]  

### Research Perspectives and Challenges
**Researcher1:** Yeah, so the main bottlenecks, I would say the development process itself. So it’s not easy during the process with these specs, the specs are changing. So in progress and that’s not easy just to coordinate new metrics. And if we have like one source of metrics, it would be easier to do. And another part is like to be of the metrics themselves. So I was trying to do it myself, but it’s really time-consuming. And so currently, PeerDAS was the first initial EIP we were trying to standardize metrics. But we’re moving further and the next one will be FOSL, which I already have something for FOSL and EIPs. As soon as they kind of, yeah, as a fight. So the bottleneck is mostly in coordination and building metrics. Yeah, because the specs are changing. This is how I see and of course the naming of the metric is also bottleneck. Because clients already have metrics named in their own way. And our goal is to make them like standardized. But it’s like our future goal. If it’s possible, if not, please share why not. Because for metrics in production is difficult. But maybe if you’re planning some big refactoring or new release, for example, where it’s possible. So we can also discuss it. Or we will have like legacy metrics like this different. But for new metrics, we will have some standardization.  
**Raúl Kripalani:** Is the naming convention for metrics captured somewhere? I see some heterogeneity (e.g., some metrics are suffixed with the unit, others aren't).  
**Justin Traglia:** [Reacted to Raúl’s message with ➕]  
**Will Corcoran:** The intention is to use Prometheus standards - J and Minny from Sunnyside suggested this, I believe.  
**Raúl Kripalani:** I’m good with Prometheus conventions (they recommend suffixing with unit name).  
**Minhyuk Kim:** I think the exact naming for each metric (like metric units / prefix/suffix) would be hard to unify across all clients because many clients have their own standard and history. But I think it’s would be useful enough if we could have the general metric naming be consistent and actually mean the same thing.  
**Researcher2:** So yeah, I just want to say that I like the idea. I do think that this would be useful. I do have a few questions. They’re like, why do you include mainnet in the name? Like are the metrics going to be different for like a minimal or other networks? What’s your idea? So how could it be like? I would just drop mainnet, just metrics.md. Unless you expect it to be different, which I don’t think it would be.  
**Katya:** Yeah, it’s like a just. It can be so here is like metrics.md. Yeah, maybe it’s still possible. It’s still the better of discussion. So just a rough idea. So yeah, we definitely can organize it like. And this is expected to live in a separate repo for this entirety. Correct. Like there’s no plan to integrate into consensus specs or execution specs. No, I just I’m currently thinking of it as a parallel process. So for example, if you’re having some changes in the specs and we’re just following or like. Maybe it’s the way to discuss. So maybe it will be from the specs initiative or from the EIP authors initiative. So what metrics they need or from researchers as well. So for developers also because if you build. Initial. I just brought the type right. You can also developers can also like. Yeah, share what metrics can be useful. So currently. I don’t see that makes sense. So that’s the right approach. Yeah, so how currently we work with FOSL. So we’ve met on the building internal and we discussed what would be nice to measure. So they already have some prototypes. In different clients. So I built initial metrics for Low Star. And I’m going to open a PR here like what metrics. I have trying to standardize like have this standardized name, which is not easy to do to be honest. But kind of and other clients can just follow. These metrics and naming and. Another part is that we’re going to build Kurtosis dashboards for this standardized metrics. And in this case, you will have a metric out of the box like dashboard out of all the box. So if you do interop locally. So Kurtosis before even devnets. You can already like see some results. So this is the idea as well.  
**Researcher2:** That sounds very useful. And then two other observations before I mute myself. I see that you named like the FOSL metrics like FOSL MD. I would just recommend using the EIP number. And then for. Like for the tables of the metrics. Like. You have a column for each client. I would recommend splitting that out into its own table because it might just get a bit unwieldy. And it’s easier just have a separate table.  
**Will Corcoran:** Would using GitBook be a viable idea?  
**Justin Traglia:** I feel that GitHub is probably the best place for this honestly. It is the common, most accessible place for devs.  
**Will Corcoran:** [Reacted to Justin’s message with 👍] Easiest to contribute to also.  
**Justin Traglia:** [Reacted to Will’s message with 👍]  
**Raúl Kripalani:** Can easily render a static site from GitHub to make it easily searchable, switch to different versions, etc.  
**Katya:** [Reacted to Raúl’s message with 👍]  
**Justin Traglia:** [Reacted to Raúl’s message with 👍] I did this for the consensus specs too: [https://jtraglia.github.io/eth-spec-viewer/](https://jtraglia.github.io/eth-spec-viewer/).  
**Raúl Kripalani:** Exactly, that just came to mind as an example ;-)  
**Will Corcoran:** Like [https://eips.ethereum.org/](https://eips.ethereum.org/)?  
**Justin Traglia:** [Reacted to Will’s and Raúl’s messages with 👍 and 😃]  
**Katya:** I actually have an idea to have like better web UI for this with filters by clients. Because of course, GitHub is like not. And MD files are like not really good for this. So these tables are huge and. Yeah, so I hope we will build some UI on top of these repos. Just for better filtering, search, etc. So the same as you have on the metrics on the consensus specs, right? So you have some UI. Yeah, I probably split out each of these separate metrics into its own subsection. And then like. You just have multiple tables and subsections. And then other notes as well. Just. So you mean not all the clients in one table. Or do you mean all the clients in one table, but separate from name type usage and collection. Yeah, I got it. Yeah, like it will be a kind of two tables. But this part and separately, this part was just the metric maybe. Yeah, sure. But anyway, yeah, like the glossary version and then the project management version below it. Yeah, we also have a glossary here, which is also like by alphabetical order. And we have a. What else we have. Oh, templates already discussed. So yeah. So the final idea is to have better UI like web UI for this because GitHub MD files are not the best way to search through the metrics because we’ll have them a lot actually. You know, every EIP may have, I don’t know like 20 metrics or depending on EIP, I’ve metrics 20 etc.  
**Raúl Kripalani:** If we want to converge to identical naming, Prometheus / OpenMetrics / OpenTelemetry implementations may support some aliasing. Or clients could register a metric under multiple registries and expose on different endpoints. But perhaps it’s more practical for existing metrics to simply retain their name and for this table to list the concrete name under the client column. For future metrics, we can try to converge.  
**Minhyuk Kim:** Ethpandaops also has etherum-metrics-exporter which can aggregate metrics under different name and re-export it as a single metric - this may be more useful for already existing metric that we want to add to the identical name metric.  
**Jae Hee Lee:** Within the table, I think it’d be nice to be more descriptive (adding examples and having as much information as possible) to mitigate different interpretations between different clients/researchers/etc.  

### Research Perspectives and Challenges
**Researcher3 (Csaba Kiraly):** Yeah, if I can put my research head on. So for the research part. I think there’s a difference between things that you just want one-off and things where you want kind of long-term monitoring of metrics. And I think kind of this same work we want for the long-term standardized metrics. So that I think would be good to have real. The other thing is that again for research. Sometimes you can just add the metric on your own. What is the help of someone like I was doing it in the sea island and guess. Yeah. But sometimes you need the metric to be widespread because you need data from mainnet from many nodes. From larger experiments. And that’s where you want to go kind of the standardization path. I think it’s important to kind of draw the line with what you want to standardize and why and I think it’s the kind of the long-term rolling. You need it from some nodes in testnets. And you want it from everyone in testnets. All eventually and mainnet. And you need it. And yeah. So you’re correct that it should be standardized for metrics which are like in use again and again by researchers. So for example, if you know that I don’t know request metrics go from one research to another. So this definitely should be standardized like or I don’t know some processes which go. So one research to another. So usually first you need it to quantify the difference in what your new proposal creates. And then actually you need the metric in both the old and the new one often. And then you need to quantify the difference. And then it becomes a kind of a quality monitoring tool. It can be going to dashboard and because we have the standardized names and the standardizing things around it definition. It’s just easier to make dashboards. Yeah, it’s official for clients. And yeah, the process won’t be so short. So it will take time of course like any new process. Yeah, I’m just thinking about. So maybe you organize it by EIP or by EIP topic. So organizing by EIP will be historically in the development part, probably. Yeah, so here if you know, low, maybe we will have some index by EIP as well. Let’s say. Yeah, it’s the matter of UI, I guess. So for example, in glossary, I added here like the status and probably EIP. That would be great to have web UI just to sort by EIP to filter by EIP. So yeah, to have different options. That’s why I’m thinking of UI better UI.  
**Katya:** Yeah, I’m just thinking what you’re gaining compared to the flat list of standardized metrics. This flat list is just for searching. I mean, I don’t know how do you when you research, how do you search for metrics? You just go through all the code or all the primitives. But no, I know I get the endpoint and just look what’s exposed. And then I have to dig into the code. Like at the moment, if you want to get a let’s say an inverse metric. You either go directly to the code or you look at dashboards, which people are prepared. And then you have to find something or you just go to the metrics endpoint of learning Nimbus and then you have the list. And then just search for something which resembles something that you are looking for. Yeah, when you have all these is just to go to the code point, well, the metric should be placed in the code. And then you see what metric is being increased. So that’s kind of I did it from the code or from some of the list of metrics. That’s how you work, right? A lot of work.  
**Researcher3 (Csaba Kiraly):** Then when I’m doing research, I almost always had to add metrics on my own. Like it was the one that we did without was the block propagation and the attestation propagation. That we did based on Saturday. But that was because the metrics were not precise because no one did precise metrics for those. It was a first approximation. And then to go more precise, we were going into the code and then logging and added the stats on top. So I see this more. One is when you have it like an idea of let’s say FOSL and then you have a few things that you want to the component track. Then a spot of your proposal you are. You have a place where you can ask basically where you can define the metrics that you will need. And then some of the clients might implement it or you implement it in some of the clients. Which is really easier. Yeah. But then as the tech as FOSL gets I don’t know which of those status it gets. It becomes part of the kind of the list of metrics that everyone is exposing. Yes, one point. Like the momentum on which one are the metrics because when you are doing the results, you are also doing a bunch of expensive metrics. So metrics have a cost often, especially the ones that you need for the search. And there is a difference between the metrics that you want to add to justify the thing. And then the metrics that you want to be the long-term maybe to just monitor the things are like going as expected. So what I expect is that when I’m working on let’s say I’m not working on FOSL research, but if I could be working on FOSL research. Then I would have this kind of wish list, right? Yes. If I. And that how cool it would be. And then either you implement for you manage some client teams implemented. And then it’s good if you have a kind of a. And then you have a structure in which you define and it’s already aligned. We kind of have it per client, but we don’t have it in the 5th, so the advantage that you get from the interface structure is that. Then when you learn bigger experiments and then when you compare how it works on mixed testnets and everything, you just get much more data points. And that’s what we’re going to have in templates. So some unified structure and you can open issue here and yeah. And then I guess there will be. I think the process would be somehow. I don’t just clients like decide what to implement it one other. You have a cycle of that. Or maybe there’s a working group. What call it what you want. That is trying to agree on which other metrics that has been implemented. Then it can be used for whatever. By-paradox. Yeah, especially in formatrix in production and yeah, such a something general. I see that more like a working group in which has a call and then they just try to agree on a few metrics which get standardizing the send it everyone is implementing it.  
**Minhyuk Kim:** @Csaba Kiraly for the metrics that raise concern for performance, how expensive are they usually?  
**Csaba Kiraly:** I have e.g. per-peer metrics. Very heavy both to process and to expose. In Nimbus there is (or was, I think it is now removed) a special flag to enable some heavy metrics.  
**Minhyuk Kim:** Yea, I think having some flag to enable verbose metric in each client would be very nice (but could be tricky to implement).  
**Csaba Kiraly:** 1 thing to add is that for research I’m often resorting to logs. In fact for some of the research I’ve implanted both logs and metrics: Logs for the research, Metrics for the approximate tracking on a dashboard.  

### Implementation and Organization Discussions
**Raúl Kripalani:** Yeah, so a bunch of thoughts here. I think. So I mentioned already in chat. I think it would be good to try to standardize the naming semantics or the naming convention for these metrics and like what I’ve generally seen in the past is something like and I think it’s implicitly applied here, but it would be good to kind of like capture it capture some rules somewhere and that would be something like domain. All the by component for the process action and then whether we use unit or not depending on. So it’s kind of like a hierarchical naming. Then this I think ties into this schema. And then general the catalog. I think we might be at this point like a bin kind of following the conversations here and I wonder if we’re trying to do too much with markdown. There is the possibility. So a couple of things here there is. The notion of versioning here, which I think is implicitly introduced on mainnet and then like other things. And there is. It’s not going to be unified across all clients. So I’m not entirely sure what the versioning is trying to capture here. I do see like a standard catalog of metrics. And then on the other hand, another list. And potentially this could be a file by per implementation that is simply enumerating what metrics are implemented by what version. And then a static site could actually cross those things very easily and tell you which clients are implementing which metrics when or which metrics are implemented by each by which clients. I think we might want to as well as kind of going back and forth from this on the chat as well around what do we do with metrics that already exist like it’s Elias and to kind of like their new name or maybe we just keep them as they are and the dictionary actually just drafts the abstract metric. And then the mapping of that metric across clients that’s probably less invasive than like going and trying to like and going and changing existing metrics. But then from now on where it’s for future metrics that are introduced maybe it just makes sense to adopt a to try to adopt a uniform naming standard within the ecosystem because it just makes things things easier. So that’s kind of just an idea here. What else I had here so when this conversation around FOSL and like as an example or PeerDAS or specific features. It’s not clear to me whether these are metrics that people working on those things are demanding from clients or from implementations or those are supposed to encapsulate metrics that are now new metrics that are relevant to the processes or the concepts that those things are introducing right so for example in the concept of peer in the in PeerDAS with PeerDAS you would probably be introducing column centric metrics which previously didn’t exist. But for PeerDAS to exist in the first place to be researched those researchers would maybe want some metrics from from the system right so. I think there is this notion of feature sets and feature flags and this also maps to some of what Java was saying around there is a cost of capturing all these metrics and I think is over time if we just end up building a list and there is no notion of feature sets. We might end up in a place where a client like the over arc expectations for client and everything but then that introduces significant observer like observe performance impact if you just offer everything. So at one point I do expect that if we start doubling down on creating more metrics will be having a lot more but then some metrics will be relevant to some users as well so clients will probably introduce mechanisms to currently bundles of those metrics on and depending on what the client needs. So if we have a client sheet the sheet can actually say group and say these are the metrics supported by this version and this is how metrics are bundled in as features that can be turned on and off. So I’m just kind of like brain dumping a little bit here I’m just like listening hopefully some of these ideas will be useful and happy to discuss them whether here or not like intending on on doing them on our log here but I’m just like just brain dumping a little bit and yeah so basically I think the take main takeaway I think we could be looking at something like YAML or something that is structured or actually storing these were actually specking out these metrics and then having a static site generated from those metrics the schema could look like metrics on one side PR is coming into those metrics the YAMLs for those metrics could be divided by domain so it’s not like one big flat YAML that develops a lot of complex over time so that maybe that is a good structure maybe there’s some hierarchical structure there. And then we think there is the notion of versioning that we can probably introduce in the schema there is the notion of implementations that will want to group things differently and then ultimately it would be having a static site generator that goes and and generates all of that into a site that’s that would be kind of like something that I think is more maintainable over time. And like what the advantage of that is that once you have this structured data you can go and add like add CI checks that are going to validate that what the changes that people are making are conforming to the schema and so on so it kind of like becomes more like it goes more on autopilot versus requiring like a manual human curator which can only happen with markdown unless you like bring in some from PR.  
**Katya:** Yeah, I agree. I was thinking also about when we have devnets and all these metrics do we need to move them into mainnet like or probably we just need to eat or to purge them because not all the networks like aren’t definitely needed.  
**Csaba Kiraly:** Network is not a dimension here. Once you have a metric implemented, it is available (eventually can be turned on) everywhere.  
**nflaig:** Just call the folder "stable"?  
**Raúl Kripalani:** What are we modelling? Something like main/dev branches?  
**Katya:** Yeah, I don’t really see the network being like a primary category of organization here. I think it’s mostly like ultimately what we might see as clients in specific versions. They just support for specific metrics and then those versions will be might be running on testnets or they’re like a one point that run on mainnet. So it is more like related with the client version than it is true to the network. The same structure with the specs right so yeah, sorry, just any we’re trying to. I was just trying to say that yeah, I mean the network shouldn’t be relevant here. It will apply to all networks. Okay. Thanks.  
**Will Corcoran:** Yeah, I guess this is more on a pragmatic level and essentially like trying to bring it to the attention of each client team like hey, do you think the best process from the people that are like myself. I think that would be to open issues in each client repo and just say hey, like these are the metrics that we hope that you track or implement for this EIP and you know, just sort of. I’m just trying to figure out like the best way to have a collaborative experience that isn’t annoying to the client teams but has some consistency and tries to get a high degree of uptake and low amount of management overhead.  
**Researcher3 (Csaba Kiraly):** So yeah, I think I already seen pushback from clients and on metrics and they do it for the reason like they have enough things to do. I think the kind of topic or the EIP related research related. Creating the explain why you would want to learn about that and then having some clients implemented. Ways is the good way and then kind of speaking a few that are standardized. And I don’t know, I’m not sure opening issues or PRs would be like that’s a good tracking but that’s I don’t think it’s good if there’s the trigger. I think that the goal should be some kind of working group or someone agreeing that that’s kind of something that everyone wants to implement and then then we just kind of have a place that we positively well we define then the metric well because there is also the kind of how well the metric is defined like if we want a metric implemented by everyone. Those few metrics we want to define well because it’s very easy to have like not the simple ones the right and no block number whatever but the more complex metrics, especially where the statistics involved you want to define well maybe captured differences where was the time exactly time is the timing related metrics, for example, need a much longer description. So in some sense it’s what the ones who personalize I think it’s better to have few metrics and discussions around those and then space in the document to capture like the definition maybe the differences in client implementation so that you know how is captured maybe like one client is deciding that they are implementing it first and then a pointer in the documentation to the actual implementation. We can call it the reference implementation of the metric or whatever. So I think that that’s simple. But you do need the kind of buy-in of the clients I think because like you know this is the next feature we want to implement to make sure that it’s working well it would be nice to measure this and that. And then I think so that would be clients which say I can actually easily add that while I’m implementing I’m also adding the metric because I’m noticing that.  
**nflaig:** Who’s responsible for keeping track of what each client implements? It might end up like [https://github.com/ethereum/beacon-APIs/blob/master/CHANGES.md](https://github.com/ethereum/beacon-APIs/blob/master/CHANGES.md).  
**Will Corcoran:** This is my primary area of interest / involvement and what I hope to talk about.  
**Justin Traglia:** “End up like this” as in missing info?  
**Will Corcoran:** [Reacted to Justin’s message with 😆]  
**Jae Hee Lee:** Would it be EthPandaOps like as we sort of are currently? (Which isn’t strictly keep tracking, but forcing client teams to implement).  
**Raúl Kripalani:** Could be automated by polling the /metrics endpoint, and updating the repo accordingly 🤖.  
**Raúl Kripalani:** Another thing: one pesky thing about normalizing metrics is that they’re only useful if they are updated under the exact trigger condition across all clients. Usually for precise research, you need high resolution datapoints, high timing precision, etc. which Prometheus metrics aren’t capable of delivering. They’re useful for a first look / approximation but this kind of metrics tends to be more useful for operational monitoring.  
**nflaig:** Tracing would give you that data, not sure if any client implements that.  
**Raúl Kripalani:** Exactly, tracing. But it’s tricky to normalize across clients because they’re so fine-grained.  
**nflaig:** Yeah probably even harder to standardize but there are open standards like OpenTelemetry.  
**Will Corcoran:** Would it ever be appropriate to have ‘must-have’ high-value metrics cross-reference / required in the specs?  
**Justin Traglia:** Yup good idea. I would probably call the not-high-value metrics auxiliary metrics.  
**Will Corcoran:** [Reacted to Justin’s message with 👍]  
**Raúl Kripalani:** Oh, I mean the exact semantics / conditions. For example, if we were to trace incoming blob transactions, do we trace before or after writing the blob to disk? If this is not defined with excruciating detail, they wouldn’t be reliable for research (but probably ok for monitoring).  
**Will Corcoran:** @Jae Hee Lee / @Minhyuk Kim can you speak to how metrics have been important - or have fallen short - in your max blobs research, etc?  
**Jae Hee Lee:** We have to plan for BPOs.  

### DevOps and Data User Insights
**Maria:** Ethpandasops website 😄 [https://ethpandaops.io/data/xatu/](https://ethpandaops.io/data/xatu/).  
**Will Corcoran:** [Reacted to Maria’s message with ❤️] This? [https://ethpandaops.io/data/history/](https://ethpandaops.io/data/history/).  
**Justin Traglia:** Not quite.  
**Maria:** [Shares link: https://ethpandaops.io/data/xatu/]  
**Will Corcoran:** [Reacted to Maria’s link with 🙏]  
**Justin Traglia:** [Reacted to Maria’s link with 👍]  
**Maria:** I just wanted to add so maybe like a slightly different perspective from like a data scientist or data user point of view. I think to me that the biggest benefit of this effort would be to have that standardized list of metrics in a way that would be easy to search with a good description of what they mean. And also potentially some links to where can I find this data is this even available like. For some metrics, maybe it went up shot to dataset will have it for others. They might be in dune or any other source. And having that like let’s say. List of what metrics can I even extract just on a first basis and then. Which of those are already being extracted and where can they be found I think would be valuable. But again, it’s like from a perspective of a data scientist that is not actually like running the clients and collecting the metrics itself. So I think that’s useful.  
**Katya:** Yeah, for this I was thinking even about like you know, boards CI boards, etc. So just monitoring clients if this client repos this metric is still exists and updating the status like something like this automated part definitely will add in future.  

### Implementation Challenges
**Manu (Nico):** Yeah, I mean just you asked what would be the for your exact term. How much work is that going to be I mean that’s kind of my question. Do you expect every single client to agree on every single metric because just getting this them to agree on anything is going to take quite some time. So I’m just wondering you know how binding this stuff will have to be because if we need to get you know, for example paradigm to agree on everything. That’s going to be difficult guess is probably it’s on you know special snowflake as well. I mean, okay, so I’m currently. Okay, it’s not me it’s Carlos doing it. I’m just speaking in his name right now. If you expect you know, we’re trying to get every client to provide just a small subset of metrics. And even that is quite difficult. It’s not like people are you know, not helpful, but there’s stuff that is really hard to get from one client that’s pretty easy to get from another. So I’m just I’m just wondering what is the. How are people expected to organize around this because I see this as being potential you know, if we want to be. To have every single metric and it being all encompassing and we want to every client to implement every metric. I think we’re going to that’s going to be a headache.  
**Katya:** Yeah, exactly. This is the main pain how to build it. This is why this is a long-term project of course. And this is why we’re starting from current development. So not to organize all the metrics at once. This is impossible. But just start building them and start sizing them from EIP to EIP. And the coordination can be well, we have this repos with some list of metrics. We have this issues of PRs and this is already makes easier just to follow what metrics we need. And we also can use maybe outsource contributors like external contributors. So like good for CI issue or need help. For example, we need this list of metrics and of course someone needs to review. But anyway, just not to waste developers time on it. Maybe it’s also like potentially can be. The help from them. Yeah, but if so metric is for example extremely expensive to get and some external contributor does it and we don’t want to merge it. Because it’s not properly done or it’s not easy to get and it’s going to kill performance. You see what I mean like I don’t want to we don’t want to feel the pressure from someone from ACD that you’re the only ones who haven’t implemented this metric. Yeah, sure, but that’s because it’s going to kill performance. You see what I mean like that’s why I’m asking how do you how do you navigate that? I think it’s impossible like to do it. Okay, like all the clients should must have this metric. But maybe at least the majority of like one to three clients have it and we already see the potential like. Sorry for God the world. I think plus pros of this and maybe just. From the EIP to EIP clients will be maybe more involved. So it’s also the process of how we’re going to. I think it’s similar to the big NAPI right where we expect Eric line to implement it at some point at least. For these optional APIs it’s a bit more difficult where you are not forced as a client to have it by the hard fork. But I think it’s good to have a similar process like every client team reviews a PR which adds metrics and then you can get approvals from each team. Or feedback if some metric is not feasible for them. And I think that usually works at least on the BKNAPI. Yeah, so it won’t be ideal like okay tomorrow Eric client has like all this metrics done but. Whenever clients got used to it yes we have metrics we have standardized metrics and you have this processes. We will try to build the process itself better so we will take time. This is why we’re here to hear from you. Yeah. So currently I guess a side side labs is trying to build metrics on a part right on the outside. Yeah, so for example I don’t know consensus clients more or less got used to me annoying with metrics. It’s just we just need some time to build this processes to optimize them. So I think it’s better for clients and not to ruin bandwidth etc.  

### Wrap-Up
**Will Corcoran:** A very prelim schedule for engaging various types of stakeholders:  
Fri, Aug 8: CL / EL client devs  
Fri, Aug 15: Dune / Nansen / PandaOps / Sunnyside  
Fri, Aug 22: skip (Ethproofs call)  
Fri, Aug 29: Tim / Stokes / Mario / Pari  
Sept: Introduce the effort at ACDT(?)  
Missing anyone?  
**nflaig:** You need to get Barnabas to bully clients if metrics are missing.  
**Justin Traglia, Will Corcoran, Minhyuk Kim, Jae Hee Lee, Manu, Raúl Kripalani:** [Reacted to nflaig’s message with 😂]  
**Raúl Kripalani:** Barnabas "the whip".  
**Katya:** This has been really valuable thank you all for your time. I dropped a little schedule you know just all the different types of stakeholders that were trying to engage in this process over the coming weeks months. I think it’s going to take a village to arrive at the Goldilocks zone of like what’s too much that we don’t want to ask too much and annoy people but we don’t want to ask too little and not to the information that we need. So stay tuned. I feel like we probably need a channel. We are going to proceed with these maybe HackMD files or PRs and we will ask for your feedback maybe we will try to re-organize it and try to ask for feedback again. So it will be iterative so I hope you will help us with this like yeah not wasting too much of your time but maybe a sync. Thank you.  
**Guillaume:** Thanks!  
**Katya:** Thank you. We will wrap up here thank you very much bye everyone.  
**Participants:** Bye everyone.